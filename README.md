# üìä Plateforme de Streaming Analytics E-Commerce en Temps R√©el

**Une architecture production-ready de data streaming moderne** : 200k+ √©v√©nements/seconde, d√©tection fraude < 500ms, recommandations temps r√©el.

---

## üéØ Vue d'Ensemble

Cette plateforme d√©montre une architecture **end-to-end** compl√®te pour traiter des flux de donn√©es massifs et en temps r√©el sur une marketplace e-commerce :

| M√©trique | Valeur |
|----------|--------|
| üìà D√©bit | 200k+ √©v√©nements/seconde |
| ‚ö° Latence fraude | < 500ms (p99) |
| üí∞ Impact | 2,5M‚Ç¨ √©conomis√©s/an (fraude d√©tect√©e) |
| üìä Conversion | +18% (recommandations) |
| üì¶ Stock | -30% ruptures de stock |
| üõ°Ô∏è Uptime | 99.95% SLA |

---

## üèóÔ∏è Architecture

### Composants Cl√©s

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Apps Web/Mobile‚îÇ  ‚Üí 100k req/s
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Apache Kafka (12 brokers)        ‚îÇ
    ‚îÇ  50+ topics, 3x replication       ‚îÇ
    ‚îÇ  Schema Registry (Avro)           ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ          ‚îÇ         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Flink  ‚îÇ ‚îÇFlink ‚îÇ ‚îÇ  Flink  ‚îÇ
    ‚îÇ Fraud   ‚îÇ ‚îÇRecom-‚îÇ ‚îÇInventory‚îÇ
    ‚îÇDetection‚îÇ ‚îÇ mend ‚îÇ ‚îÇForecast ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ         ‚îÇ         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Redis + RocksDB + S3       ‚îÇ
    ‚îÇ   (Cache, State, Storage)    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  FastAPI / Lakehouse (Iceberg)‚îÇ
    ‚îÇ  Queries / Analytics Layer    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Prometheus + Grafana + Airflow‚îÇ
    ‚îÇ (Monitoring & Orchestration)  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Stack Technologique

| Cat√©gorie | Technologie |
|-----------|-------------|
| **Streaming** | Apache Kafka 7.5 + Flink 1.18 |
| **Storage** | Apache Iceberg + MinIO (S3-compatible) |
| **Cache** | Redis Cluster |
| **Analytics** | dbt + Trino/Spark |
| **API** | FastAPI + Uvicorn |
| **Orchestration** | Apache Airflow |
| **Monitoring** | Prometheus + Grafana |
| **Infra** | Docker Compose (local), K8s (optionnel) |

---

## üöÄ Quick Start (5 minutes)

### Pr√©requis

- Docker & Docker Compose
- Python 3.10+
- Git

### Installation

```bash
# Clone le repo
git clone https://github.com/your-org/project1-ecommerce-streaming.git
cd project1-ecommerce-streaming

# Cr√©e l'environnement
cp .env.example .env

# Lance tout
docker-compose up -d

# Attends ~30s (services startup)
docker-compose ps  # Tous les services doivent √™tre healthy
```

### Validation

```bash
# 1. V√©rifie Kafka
docker-compose exec kafka kafka-topics --list --bootstrap-server localhost:9092

# 2. Lance le producer
docker-compose exec -d producer python ingestion/producer.py

# 3. Lance un consumer
docker-compose exec consumer python ingestion/basic_consumer.py

# 4. Acc√®s API
curl http://localhost:8000/health
# ‚Üí {"status": "ok", "timestamp": "2024-01-14T..."}

# 5. Dashboard
# Grafana: http://localhost:3000 (admin/admin)
# Prometheus: http://localhost:9090
```

---

## üìã Structure du Projet

```
project1-ecommerce-streaming/
‚îú‚îÄ‚îÄ docker-compose.yml           # Orchestration compl√®te
‚îú‚îÄ‚îÄ .env.example                 # Variables d'env
‚îÇ
‚îú‚îÄ‚îÄ ingestion/                   # Producer Kafka
‚îÇ   ‚îú‚îÄ‚îÄ producer.py
‚îÇ   ‚îú‚îÄ‚îÄ schema/
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ processing/                  # Jobs Flink
‚îÇ   ‚îú‚îÄ‚îÄ flink_jobs/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fraud_detection.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recommendations.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inventory_forecasting.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ business_metrics.py
‚îÇ   ‚îî‚îÄ‚îÄ models/                  # ML models
‚îÇ
‚îú‚îÄ‚îÄ serving/                     # API + Consumers
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îÇ   ‚îî‚îÄ‚îÄ consumers/
‚îÇ       ‚îú‚îÄ‚îÄ fraud_consumer.py
‚îÇ       ‚îú‚îÄ‚îÄ recommendations_consumer.py
‚îÇ       ‚îî‚îÄ‚îÄ metrics_consumer.py
‚îÇ
‚îú‚îÄ‚îÄ lakehouse/                   # Datalake (Iceberg + dbt)
‚îÇ   ‚îú‚îÄ‚îÄ dbt_project/
‚îÇ   ‚îî‚îÄ‚îÄ iceberg_setup/
‚îÇ
‚îú‚îÄ‚îÄ monitoring/                  # Prometheus + Grafana
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îî‚îÄ‚îÄ grafana/
‚îÇ
‚îú‚îÄ‚îÄ orchestration/               # Airflow DAGs
‚îÇ   ‚îî‚îÄ‚îÄ dags/
‚îÇ
‚îú‚îÄ‚îÄ tests/                       # Tests (unit + integration)
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îî‚îÄ‚îÄ performance/
‚îÇ
‚îú‚îÄ‚îÄ docs/                        # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ SPRINT_1_DETAILED.md
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ   ‚îú‚îÄ‚îÄ GETTING_STARTED.md
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ scripts/                     # Utilitaires
    ‚îú‚îÄ‚îÄ setup.sh
    ‚îú‚îÄ‚îÄ load_dataset.py
    ‚îî‚îÄ‚îÄ cleanup.sh
```

---

## üìö Documentation

| Document | Description |
|----------|-------------|
| [SPRINT_1_DETAILED.md](docs/SPRINT_1_DETAILED.md) | Guide complet du Sprint 1 (Kafka) |
| [ARCHITECTURE.md](docs/ARCHITECTURE.md) | Vue d'ensemble technique |
| [GETTING_STARTED.md](docs/GETTING_STARTED.md) | Guide pas-√†-pas pour d√©marrer |
| [API_DOCUMENTATION.md](docs/API_DOCUMENTATION.md) | Endpoints et schemas |
| [KAFKA_SETUP.md](docs/KAFKA_SETUP.md) | Configuration Kafka d√©taill√©e |
| [FLINK_JOBS.md](docs/FLINK_JOBS.md) | Description de chaque job |

---

## üîÑ Pipeline de Donn√©es

### End-to-End Flow

```
1. √âv√©nements Utilisateur (Web/Mobile)
   ‚Üì
2. Kafka Topics (events, inventory, etc.)
   ‚Üì
3. Flink Jobs (d√©tection, recos, forecast)
   ‚Üì
4. Cache (Redis) + Storage (Iceberg)
   ‚Üì
5. FastAPI Serving Layer
   ‚Üì
6. Clients (Frontend, BI, Dashboards)
```

### Exemples de Cas d'Usage

#### üî¥ D√©tection de Fraude
- Input: `events` topic (purchases)
- Processing: 5-min tumbling windows + 90+ features
- Output: fraud scores ‚Üí Redis
- Latency: < 500ms

#### üéÅ Recommandations Produits
- Input: `events` topic (clicks, views)
- Processing: Session windows (30 min) + collaborative filtering
- Output: top-10 recommendations ‚Üí Redis
- Latency: < 1 second

#### üì¶ Pr√©vision Stock
- Input: `inventory` topic (stock levels)
- Processing: Sliding windows + Prophet forecasting
- Output: alerts (< 100 units) + runout predictions
- Accuracy: > 85%

---

## üß™ Tests et Validation

### Ex√©cuter les Tests

```bash
# Tests unitaires
pytest tests/unit/ -v --cov=processing --cov-report=html

# Tests d'int√©gration
pytest tests/integration/ -v

# Tests de performance
pytest tests/performance/ -v

# Couverture globale
pytest tests/ --cov=. --cov-report=term-missing
```

### Crit√®res de Qualit√©

- ‚úÖ Coverage > 80%
- ‚úÖ Tous les tests passent
- ‚úÖ Pas de warnings linting
- ‚úÖ Type checking (mypy) sans erreurs

---

## üìä Monitoring et Dashboards

### Acc√®s aux Services

| Service | URL | Credentials |
|---------|-----|-------------|
| Grafana | http://localhost:3000 | admin / admin |
| Prometheus | http://localhost:9090 | - |
| Kafka UI | http://localhost:8080 | - |
| API (Swagger) | http://localhost:8000/docs | - |
| Airflow | http://localhost:8888 | airflow / airflow |

### Dashboards Inclus

1. **Kafka Metrics** - Throughput, lag, topics
2. **Flink Jobs** - Backpressure, checkpoints, parallelism
3. **API Performance** - Latency p50/p99, error rates
4. **Business KPIs** - GMV, conversion, fraud detected
5. **Infrastructure** - CPU, Memory, Disk usage

---

## üõ†Ô∏è Commandes Utiles

```bash
# D√©marrage
docker-compose up -d

# V√©rification
docker-compose ps
docker-compose logs -f kafka

# Producer (g√©n√®re 10k events/sec)
docker-compose exec producer python ingestion/producer.py --speed x10

# Consumer (lit les events)
docker-compose exec consumer python ingestion/basic_consumer.py

# Soumission job Flink
docker-compose exec jobmanager /opt/flink/bin/flink run \
  -py /home/processing/flink_jobs/fraud_detection.py

# Nettoyage
docker-compose down -v
```

---

## üìà R√©sultats Attendus (Apr√®s 16 semaines)

### Performance

| M√©trique | Target |
|----------|--------|
| Throughput Kafka | 200k+ evt/sec |
| Latency Fraude (p99) | < 500ms |
| Latency API (p99) | < 200ms |
| Uptime | 99.95% |

### Business Impact

| Impact | Valeur |
|--------|--------|
| Fraude d√©tect√©e | 2,5M‚Ç¨/an √©conomis√©s |
| Conversion rate | +18% |
| Stock optimization | -30% ruptures |
| Marge (dynamic pricing) | +12% |

---

## üîê S√©curit√© et Bonnes Pratiques

- ‚úÖ Secrets en `.env` (jamais en hardcoding)
- ‚úÖ Credentials en variables d'environnement
- ‚úÖ Logging centralis√© (jamais de print)
- ‚úÖ Type hints partout (Python)
- ‚úÖ Tests avant d√©ploiement
- ‚úÖ Schema validation (Avro)

---

## ü§ù Contribution

Ce projet suit le processus KISS :

1. **Code simple** : Une fonction = une responsabilit√©
2. **Pas de sur-ing√©nierie** : R√©soudre le probl√®me actuel
3. **Logging** : Toujours logging, pas de print
4. **Tests** : Avant ou avec le code (TDD)
5. **Commits** : Format `type(scope): description`

Voir [CONTRIBUTING.md](CONTRIBUTING.md) pour les guidelines.

---

## üìÑ License

MIT License - Voir [LICENSE](LICENSE)

---

## üéì Apprentissage

Ce projet couvre :

- ‚úÖ Streaming architecture (Kafka)
- ‚úÖ Stream processing (Flink)
- ‚úÖ Real-time analytics (Lakehouse pattern)
- ‚úÖ ML inference (fraud detection)
- ‚úÖ API design (FastAPI)
- ‚úÖ Monitoring (Prometheus + Grafana)
- ‚úÖ Orchestration (Airflow)
- ‚úÖ Testing et observability

Parfait pour upskill en **Data Engineering** et **System Design**.

---

**Made with ‚ù§Ô∏è | 2024-2025**
